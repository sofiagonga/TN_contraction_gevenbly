{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial: Network Contractions.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOqvsMy9VChA"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Intro to Tensor Network Contractions\n",
        "by Glen Evenbly, [glenevenbly.com](https://glenevenbly.com)\n",
        "\n",
        "In this tutorial you will learn the basics of performing tensor network contractions in python. Topics include:\n",
        "- the computational costs of network contractions\n",
        "- network contractions via a sequence of pairwise contractions\n",
        "- optimal contraction orders\n",
        "- use of `xcon` function for contracting networks\n",
        "- calculation of environments (or single tensor gradients) from closed networks\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0O1ldosUs3Q"
      },
      "source": [
        "# Install glens tensor algorithm library\n",
        "!git clone https://github.com/gevenbly/TensorAlgs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvGARWTJfLBK"
      },
      "source": [
        "# Import necessary modules\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "os.chdir('/content/TensorAlgs')\n",
        "# %run ./network_render.ipynb\n",
        "# %run ./network_contract.ipynb\n",
        "from network_contract import (\n",
        "    check_network, compute_costs, remove_tensor, solve_order, ncon, xcon)\n",
        "from network_render import draw_network, draw_network_interactive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0346ytFleACX"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Network contraction via index summation\n",
        "\n",
        "Assume that we wish to contract a simple network composed of three tensors $\\{A, B, C\\}$ defined as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "{D_{inm}} = \\sum\\limits_{jkl} {{A_{ijk}}{B_{klm}}{C_{jnl}}} \\tag{Eq.1}\n",
        "\\end{equation}\n",
        "\n",
        "This network can be represented diagramatically as:\n",
        "<a name=\"fig1\"></a>\n",
        "<p align=\"center\">\n",
        "<img src='https://github.com/gevenbly/TensorAlgs/blob/main/images/net3.png?raw=true' width=500px $\\textrm{Fig}.(1)$ > \n",
        "</p> \n",
        "<p align = \"right\">\n",
        "$(\\textrm{Fig}.1)$ \n",
        "</p>\n",
        "\n",
        "The simplest way to evaluate this contraction is using a set of nested for loops, with a loop for each index involved in the contraction, as shown below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUrbfnb6fBFI"
      },
      "source": [
        "# Snip.(1): network contraction via index summation\n",
        "\n",
        "# define some random tensors\n",
        "d = 5\n",
        "A = np.random.rand(d,d,d)\n",
        "B = np.random.rand(d,d,d)\n",
        "C = np.random.rand(d,d,d)\n",
        "\n",
        "# evaluate using explicit index summation\n",
        "D = np.zeros((d,d,d), dtype=float)\n",
        "for i in range(d):\n",
        "  for n in range(d):\n",
        "    for m in range(d):\n",
        "      for j in range(d):\n",
        "        for k in range(d):\n",
        "          for l in range(d):\n",
        "            D[i,n,m] += A[i,j,k]*B[k,l,m]*C[j,n,l]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi_lXZitGS6Q"
      },
      "source": [
        "While producing the correct result, the evaluation using index summation has several issues. \n",
        "\n",
        "Firstly it is verbose to code a contraction in this way, especially for networks beyond the very simple network considered in the example. A sophisticated tensor algorithm may involve networks with dozens of tensors and hundreds of indices (and coding `for` loops nested 100+ deep would not be good practice!).\n",
        "\n",
        "Secondly, and perhaps more significantly, evaluation using index summation is also computationally inefficient. The cost of running the code snippet above scales as $d^6$ (in terms of the number of scalar multiplications), however, as we shall see shortly, the same contraction can also be performed with cost $2d^5$ when using a different method.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvtGcaOnIU19"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Pairwise tensor contractions and computational costs\n",
        "\n",
        "A better way to contract a tensor network containing $N>2$ tensors is through a sequence of $(N-1)$ pairwise tensor contractions; this is usually substantially more computationally efficient explicit index summation.\n",
        "\n",
        "A pairwise contraction between two tensors $A$ and $B$ involves the summation over all indices shared between these two tensors, the cost of which (in terms of scalar multiplications) follows from that of the standard matrix multiplication algorithm,\n",
        "\n",
        "<a name=\"eq2\"></a>\n",
        "\\begin{equation}\n",
        "\\textrm{cost}:(A \\times B) = \\frac{{\\left\\| A \\right\\|\\left\\| B \\right\\|}}{{\\left\\| {A \\cap B} \\right\\|}}, \\tag{Eq.2}\n",
        "\\end{equation}\n",
        "\n",
        "with $\\left\\| A \\right\\|$ and $\\left\\| B \\right\\|$ as the total dimension of each tensor, and ${{\\left\\| {A \\cap B} \\right\\|}}$ as the total dimension of the indices shared between the two tensors. Following this equation, and assuming indices of dimension $d$, the cost of a vector-matrix multiplication evaluates as $d^2$, while the cost of a matrix-matrix multiplication evaluates as $d^3$, as expected.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdU2HUpsZWOy"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "**Example 1: sequences of pairwise contractions**\n",
        "\n",
        "Consider the network of [Fig.1](#fig1), which we previously evaluated using index summation. Instead we shall now evaluate this network with a sequence of pairwise contractions, first contracting $A$ with $B$ and then contracting the product with tensor $C$. Assuming all indices are $d$ dimensional and following [Eq.2](#eq2) the contraction costs evaluate as\n",
        "\n",
        "\\begin{align}\n",
        "\\textrm {cost}& :(A \\times B) = d^5\\\\\n",
        "\\textrm {cost}& :(\\{A B\\} \\times C) = d^5 \\tag{Eq.3}\n",
        "\\end{align}\n",
        "\n",
        "where we use $\\{A B\\}$ to denote the tensor resulting from the product of $A$ and $B$. Thus the total contraction cost is $2d^5$, which is computationally cheaper than index summation (at total cost $d^6$) for all index dimensions $d>2$.\n",
        "\n",
        "In practice we can use the numpy `tensordot` function to evaluate a pairwise contraction, as demonstrated in the snippet below for the evaluation of [Fig.1](#fig1). Note that one has to be careful with indew ordering; here we require explicit use of `transpose` in order to maintain an index ordering consistant with that defined in [Eq.1](#eq2).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvEzIeNEQkdC"
      },
      "source": [
        "# Snip.(2): network contraction of Fig.1 via pairwise contractions\n",
        "\n",
        "# define some random tensors\n",
        "d = 5\n",
        "A = np.random.rand(d,d,d)\n",
        "B = np.random.rand(d,d,d)\n",
        "C = np.random.rand(d,d,d)\n",
        "\n",
        "# evaluate using explicit index summation\n",
        "D = np.zeros((d,d,d), dtype=float)\n",
        "for i in range(d):\n",
        "  for n in range(d):\n",
        "    for m in range(d):\n",
        "      for j in range(d):\n",
        "        for k in range(d):\n",
        "          for l in range(d):\n",
        "            D[i,n,m] += A[i,j,k]*B[k,l,m]*C[j,n,l]\n",
        "\n",
        "# evaluate using sequence of tensordot\n",
        "AB = np.tensordot(A, B, axes=((2), (0)))\n",
        "D0 = np.tensordot(AB, C, axes=((1,2), (0,2))).transpose(0,2,1)\n",
        "\n",
        "# check that the two results match\n",
        "print('tensors match: %r' %(np.allclose(D,D0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S89_VDQzgRn"
      },
      "source": [
        "while use of `tensordot` to contract a network is certainly an improvement over the index summation we first considered, it is still unsatisfactory for a number of reasons. \n",
        "\n",
        "Firstly, for larger networks (e.g. containing 10+ tensors) it is still going to be exceptionally challenging to write the contraction as a sequence of `tensordot` operations, so we would still like to be able to use a more user-friendly and readible syntax.\n",
        "\n",
        "Secondly is a more subtle issue; the use of `tensordot` requires hardcoding a specific contraction order. For instance in Snip.(2) above we evaluated the network as $((A\\times B) \\times C)$. However if we decided instead that we wanted a different order, e.g. $((A\\times C) \\times B)$, then the code would have to be rewritten. This is important since, as we explain in the next section, the cost of a network contraction depends on the order of contraction. Thus it is highly desirable that we avoid having to hardcode specific contraction orders, i.e. that we can change contraction order without having to rewrite code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BcDGL51vMEw"
      },
      "source": [
        "---\n",
        "\n",
        "## Contraction orders\n",
        "\n",
        "When evaluation a network containing $N>2$ tensors there are multiple different sequences of pairwise contractions that could be used. While the final resulting tensor is, of course, **independent** of the particular sequence of pairwise contractions used to produce it, the total contraction cost can depend on the chosen sequence. For instance assume that we wish to evaluate the product $(A\\times B \\times v)$, with $A$, $B$ as matrices and $v$ as a vector, all with index dimensions $d$. There are two different sequences that we could use to perform the contraction, each with different total costs,\n",
        "\n",
        "\\begin{align}\n",
        "\\textrm{cost}& :((A \\times B) \\times v) = {d^3} + {d^2},\\\\\n",
        "\\textrm{cost}& :(A \\times (B \\times v)) = {2 d^2}. \\tag{Eq.4}\n",
        "\\end{align}\n",
        "\n",
        "Thus for all $d>2$ we see that the second option, $(A \\times (B \\times v))$, is the most efficient in this example.\n",
        "\n",
        "In general, we would always like to indentify and use the **optimal** contraction order (i.e. that with the smallest total computational cost over all possible contraction orders). For relatively small networks it may be possible to determine the optimal order through manual inspection. However, determining the optimal order in larger networks is a difficult task, one for which there is no known algorithm that scales polynomially with the number of tensors in the network. Later in this tutorial we will introduce numerical routines for solving for the optimal order, which are based on brute force searches aided with heuristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHtRkhucCe_p"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "**Example 2: optimal contract orders**\n",
        "\n",
        "Let us consider a contraction of a network of three tensors,\n",
        "\n",
        "\\begin{equation}\n",
        "{D_{inm}} = \\sum\\limits_{jkl} {{A_{ijk}}{B_{klm}}{C_{jnl}}} \\tag{Eq.5},\n",
        "\\end{equation}\n",
        "but now assuming that indices $\\{i,j,l,m \\}$ are dimension $d$, and indices $\\{k,n\\}$ are dimension $\\chi$. This network can be represented diagramatically as:\n",
        "<a name=\"fig2\"></a>\n",
        "<p align=\"center\">\n",
        "<img src='https://github.com/gevenbly/TensorAlgs/blob/main/images/net5.png?raw=true' width=500px > \n",
        "</p> \n",
        "<p align = \"right\">\n",
        "$(\\textrm{Fig}.2)$ \n",
        "</p>\n",
        "with black indices of dim $d$ and green indices of dim $\\chi$. Let us examine the costs of evaluating this network,\n",
        "\n",
        "\\begin{align}\n",
        "\\textrm{cost}& :((A \\times B) \\times C) = {2 d^4 \\chi},\\\\\n",
        "\\textrm{cost}& :((B \\times C) \\times A) = {2 d^3 \\chi^2}, \\\\\n",
        "\\textrm{cost}& :((A \\times C) \\times B) = {2 d^3 \\chi^2}. \\tag{Eq.6}\n",
        "\\end{align}\n",
        "\n",
        "This example demonstrates an intersting point: the optimal contraction order depends on not only the geometry of the network but also on the relative dimensions of the network indices. For $d<\\chi$ the first order is the most efficient, while for $d>\\chi$ the second or third order are better.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALS8C7igH2LF"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Advanced Contractions Using XCON\n",
        "\n",
        "In this section we demonstrate the usage of the `xcon` function (Xterme CONtractor) for contracting tensor networks. Internally, this function contracts networks via a sequence of pairwise contractions using `tensordot`, as we discussed previously in this tutorial. However use `xcon` confers several advantages over direct use of repeated `tensordot` such as:\n",
        "\n",
        "- providing a more readible and compact means of performing efficient tensor network contractions. \n",
        "\n",
        "- identifying common user-errors when defining a tensor networks, thus providing valuable debugging assistance. \n",
        "\n",
        "- allowing the contraction order for a network to be easily changed, since order is passed to `xcon` as a variable rather than being hardcoded.\n",
        " \n",
        "Note that the `xcon` network contractor is based upon the previous `ncon` contractor (Network CONtractor), and reproduces its core, functionality, but improves upon it in many significantly ways including: \n",
        "\n",
        "*   networks can now be defined with labels as `str`'s or as `int`'s.   \n",
        "*   a solver for optimal contaction order is incorporated.\n",
        "*   single tensor environments can now be automatically generated from a closed network (achieved through a smart reordering of the contraction order).  \n",
        "*   intermediate terms from a contraction are automatically recycled when evaluting multiple environments from a network, greatly improving the computational efficiency as compared to an evaluation of each environment singularly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eiQh3StqbQV"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Syntax for defining networks\n",
        "\n",
        "In order to contract the network using `ncon` we must provide the following inputs:\n",
        "- `tensors`: a list of the tensors involved\n",
        "- `connects`: a list of sublists, where each sublist specifies the indices appearing on a tensor (either a `str` or `int` label)\n",
        "- `open_order`: (optional) a list specifying the order of the open indices that appear on the final tensor. If not provided, then `xcon` will default to that provided by `list.sort()` on the open indices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqTUDYTYoV1N"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "**Example 3: contractions using xcon**\n",
        "\n",
        "Let us again consider a simple network composed of three tensors: \n",
        "\n",
        "\\begin{equation}\n",
        "{D_{inm}} = \\sum\\limits_{jkl} {{A_{ijk}}{B_{klm}}{C_{jnl}}}, \\tag{Eq.7}\n",
        "\\end{equation}\n",
        "\n",
        "which can be represented diagramatically as:\n",
        "<a name=\"fig3\"></a>\n",
        "<p align=\"center\">\n",
        "<img src='https://github.com/gevenbly/TensorAlgs/blob/main/images/net3.png?raw=true' width=500px > \n",
        "</p> \n",
        "<p align = \"right\">\n",
        "$(\\textrm{Fig}.3)$ \n",
        "</p>\n",
        "\n",
        "An example of the usage of `xcon` for contracting Fig.(3) is provided below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siPp9RbfibD1"
      },
      "source": [
        "# Snip.(3): contraction of Fig.(3) via xcon\n",
        "\n",
        "# define some random tensors\n",
        "d = 5\n",
        "A = np.random.rand(d,d,d)\n",
        "B = np.random.rand(d,d,d)\n",
        "C = np.random.rand(d,d,d)\n",
        "\n",
        "# contraction via index summation\n",
        "D = np.zeros((d,d,d), dtype=float)\n",
        "for i in range(d):\n",
        "  for n in range(d):\n",
        "    for m in range(d):\n",
        "      for j in range(d):\n",
        "        for k in range(d):\n",
        "          for l in range(d):\n",
        "            D[i,n,m] += A[i,j,k]*B[k,l,m]*C[j,n,l]\n",
        "\n",
        "# contraction via xcon\n",
        "tensors = [A,B,C]\n",
        "connects = [['i','j','k'], ['k','l','m'], ['j','n','l']]\n",
        "open_order = ['i','n','m']\n",
        "D0 = xcon(tensors, connects, open_order=open_order)\n",
        "\n",
        "# check that the two methods produce the same result\n",
        "print('tensors match: %r' %(np.allclose(D,D0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIAE0ytSQ9Kt"
      },
      "source": [
        "Notice that the index labels $\\{i,j,k,\\ldots \\}$ used to define the contracion in Eq.(7) are **dummy** indices; they have no specific meaning in themselves, their function is simply to denote which pairs of indices are contracted together. Thus we could potentially rename these indices to whatever we like without changing the definition of the network itself. \n",
        "\n",
        "One common convention (which was **required** by the previous `ncon` routine) is to label the contracted (or internal) indices of a network with positive integers $[1,2,3,\\ldots]$ and the open (or external) indices of a network with (semi) negative integers $[-0,-1,-2,\\ldots]$ in the order in which they appear on the final tensor. The use of these **standardized** labels has the advantage in that it removes the need for the `open_order` input, since this information is instead encoded by the order of the negative integers. Below we demonstrate these different conventions for the contraction of Fig.(3).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRsapodBQkdJ"
      },
      "source": [
        "# Snip.(4): contraction of Fig.(3) via xcon\n",
        "tensors = [A,B,C]\n",
        "\n",
        "# contraction using original index labels\n",
        "connects = [['i','j','k'], ['k','l','m'], ['j','n','l']]\n",
        "open_order = ['i','n','m']\n",
        "D0 = xcon(tensors, connects, open_order=open_order)\n",
        "\n",
        "# contraction using standardized external indices (no `open_order` needed!)\n",
        "connects = [[-0,'j','k'], ['k','l',-2], ['j',-1,'l']]\n",
        "D1 = xcon(tensors, connects)\n",
        "\n",
        "# contraction using standardized external and internal indices\n",
        "connects = [[-0,1,3], [3,2,-2], [1,-1,2]]\n",
        "D2 = xcon(tensors, connects)\n",
        "\n",
        "# check that the two methods produce the same result\n",
        "print('tensors match: %r' %(np.allclose(D0,D1) and np.allclose(D0,D2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jgb4QtZE0mg"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "**Example 4: partial traces and outer products**\n",
        "\n",
        "The `xcon` routine can also be used for networks containing partial traces and tensor products, thus can also effectivly replace such numpy functions such as `np.outer` and `np.kron`. We demonstrate this functionality with a simple example network:\n",
        "\n",
        "\\begin{equation}\n",
        "{D_{jlkn}} = \\sum\\limits_{im} {{A_{ijik}}{B_{lm}}{C_{mn}}}, \\tag{Eq.8}\n",
        "\\end{equation}\n",
        "\n",
        "which is represented diagramatically as\n",
        "\n",
        "<a name=\"fig4\"></a>\n",
        "<p align=\"center\">\n",
        "<img src='https://github.com/gevenbly/TensorAlgs/blob/main/images/net6.png?raw=true' width=300px $\\textrm{Fig}.(4)$ > \n",
        "</p> \n",
        "<p align = \"right\">\n",
        "$(\\textrm{Fig}.4)$ \n",
        "</p>\n",
        "\n",
        "and provide the `xcon` usage below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xeeriAvngLd"
      },
      "source": [
        "# Snip.(5): partial traces and outer products\n",
        "\n",
        "# define some random tensors\n",
        "d = 5\n",
        "A = np.random.rand(d,d,d,d)\n",
        "B = np.random.rand(d,d)\n",
        "C = np.random.rand(d,d)\n",
        "\n",
        "# evaluate using explicit index summation\n",
        "D = np.zeros((d,d,d,d), dtype=float)\n",
        "for j in range(d):\n",
        "  for l in range(d):\n",
        "    for k in range(d):\n",
        "      for n in range(d):\n",
        "        for i in range(d):\n",
        "          for m in range(d):\n",
        "            D[j,l,k,n] += A[i,j,i,k]*B[l,m]*C[m,n]\n",
        "\n",
        "# evaluate using xcon\n",
        "tensors = [A,B,C]\n",
        "connects = [['i','j','i','k'], ['l','m'], ['m', 'n']]\n",
        "open_order = ['j', 'l', 'k', 'n']\n",
        "D0 = xcon(tensors, connects, open_order=open_order)\n",
        "\n",
        "# check that the two methods produce the same result\n",
        "print('tensors match: %r' %(np.allclose(D,D0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjqV0UwOe5CV"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Contraction orders and costs\n",
        "\n",
        "Internally the `xcon` routine contracts networks through a sequence of pairwise tensor contractions. There are three options determining the order that `xcon` can use:\n",
        "- one can provide an `order` input, consisting of a list of the internal indices in the order that they should be contracted.\n",
        "- one can input a `solver` argument to call the inbuilt solver, which can automatically determine an efficient contraction order. The are several options how the solver can be employed:\n",
        "\n",
        " 1. setting `solver='full'` will search over all viable orders to provide the gauranteed optimal order. This can require significant time for larger networks (i.e. 12 or more tensors). \n",
        "\n",
        " 2. setting `solver='greedy'` will search only the locally optimal contraction orders (i.e. using a greedy algorithm). This search is always essentially instantaneous, but is not gauranteed to produce an optimal order. \n",
        "\n",
        " 3. setting `solver` as equal to an integer will use the integer value as an upper bound for the number of branches searched at each step of the search algorithm, which serves to allow itermediate settings between `'greedy'` and `'full'`. In the limit that the integer value is taken to $1$ the `'greedy'` result is reproduced, while in the limit of a large value, i.e. $O(exp(N))$ with $N$ the number tensors in the network, the `'full'` result is reproduced. \n",
        "\n",
        "- one can omit both an `order` and a `solver` argument, in which case `xcon` defaults to an order based on a `list.sort` of the internal index labels.\n",
        "\n",
        "Note that, by setting the optional `xcon` argument `return_info=True`, then `xcon` will return the contraction `order` and the computational `cost` of the network contraction in addition to the tensor from the contracted network. The return of the `order` can be used to avoid having to call the `solver` multiple times on the same network, which is otherwise inefficient.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzebce1Mo1A_"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "**Example 5: optimal contract orders**\n",
        "\n",
        "Let us reconsider the network from Example 2,\n",
        "\n",
        "\\begin{equation}\n",
        "{D_{inm}} = \\sum\\limits_{jkl} {{A_{ijk}}{B_{klm}}{C_{jnl}}} \\tag{Eq.8},\n",
        "\\end{equation}\n",
        "where indices $\\{i,j,l,m \\}$ are dimension $d$, and indices $\\{k,n\\}$ are dimension $\\chi$. This network is represented diagramatically as:\n",
        "<a name=\"fig4\"></a>\n",
        "<p align=\"center\">\n",
        "<img src='https://github.com/gevenbly/TensorAlgs/blob/main/images/net5.png?raw=true' width=500px > \n",
        "</p> \n",
        "<p align = \"right\">\n",
        "$(\\textrm{Fig}.4)$ \n",
        "</p>\n",
        "\n",
        "with black indices of dim $d$ and green indices of dim $\\chi$. Recall from Eq.(6) that for $d<\\chi$ the optimal contraction order was found to be $((A \\times B) \\times C)$ which had cost ${2 d^4 \\chi}$. \n",
        "\n",
        "This order can be input into `xcon` by setting `order = ['k', 'j', 'l']`, which informs that index $k$ should be the first to be contracted, followed by $j$ and $l$. Note that `xcon` always simultaneously contracts all of indices shared between the two tensors in a pairwise contraction. Thus setting `order = ['k', 'l', 'j']` would also be exactly equivalent, since in the contraction of the group $\\{ AB \\}$ with $C$ both indices $j$ and $l$ are contracted simultaneously.\n",
        "\n",
        "We demonstrate below how the known order can be directly set in `xcon` and, alternatively, how the solver could instead be used to determine the order.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgJ2kKj-k7ye"
      },
      "source": [
        "# Snip.(6): efficient contraction order\n",
        "\n",
        "# generate the tensors\n",
        "d = 5\n",
        "chi = 10\n",
        "A = np.random.rand(d,d,chi)\n",
        "B = np.random.rand(chi,d,d)\n",
        "C = np.random.rand(d,chi,d)\n",
        "tensors = [A,B,C]\n",
        "\n",
        "# ------------------------------\n",
        "# (a) Example: contract using known order\n",
        "connects = [[-1,'j','k'], ['k','l',-3], ['j',-2,'l']]\n",
        "order = ['k', 'j', 'l']\n",
        "D0, order0, cost0 = xcon(tensors, connects, order=order, return_info=True)\n",
        "\n",
        "# we can check that the cost matches the theoretical prediction\n",
        "cost_exact = int(2*(d**4)*chi)\n",
        "print('actual cost: %d, theory cost: %d' %(cost0, cost_exact))\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# (b) Example: contract using implicitly defined order: `order = [1,2,3]`\n",
        "connects = [[-1,2,1], [1,3,-3], [2,-2,3]]\n",
        "D2 = xcon(tensors, connects)\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# (c) Example: use solver to find the optimal order\n",
        "connects = [[-1,'j','k'], ['k','l',-3], ['j',-2,'l']]\n",
        "D1, order1, cost1 = xcon(tensors, connects, solver='full', return_info=True)\n",
        "\n",
        "# we can check that the cost matches the theoretical prediction\n",
        "cost_exact = int(2*(d**4)*chi)\n",
        "print('actual cost: %d, theory cost: %d' %(cost1, cost_exact))\n",
        "\n",
        "# check that the output order matches our expectations\n",
        "print('order determined from solver: ', order1) \n",
        "\n",
        "# note that the output order can be reused for effecient future contractions\n",
        "D1 = xcon(tensors, connects, order=order1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBEZMxi2rYFY"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Use of the order solver\n",
        "\n",
        "The solver for optimal contraction order can also be used separately from `xcon` by calling `solve_order`, which takes similar inputs to `xcon` but only finds the `order` rather than contracting a network. This can be useful, for instance, in exploring the contraction costs in the design of a new tensor network algorithm. \n",
        "\n",
        "The `solve_order` routine takes input of `tensors` and `connects` just as `xcon` does, however the `tensors` can here be either the list of tensors as usual **or** it can be a list of the shapes of each tensor (i.e. as tuples). An optional integer `max_branch` can be provided in order to limit the scope of the search: \n",
        "- omitting `max_branch` reproduces the `solver='full'` search from `xcon`.\n",
        "- setting `max_branch=1` reproduces the `solver='greedy'` search from `xcon`.  \n",
        "\n",
        "In addition to the `order` the solver also returns the ($\\textrm{log}_{10}$ of) the contraction cost and a boolean flag indicating whether the returned order in guaranteed optimal (which may be `False` if the `max_branch` is set too small).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH9y3YJ5lO2C"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "**Example 6: order solver**\n",
        "\n",
        "Let us reconsider the network from Example 5 using the `solve_order` routine:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEtMa_8TrPKp"
      },
      "source": [
        "# Snip.(7): solve for the order\n",
        "\n",
        "# set the dims\n",
        "d = 5\n",
        "chi = 10\n",
        "dims = [(d,d,chi), (chi,d,d), (d,chi,d)]\n",
        "\n",
        "# ------------------------------\n",
        "# (a) Example: contract using known order\n",
        "connects = [[-1,'j','k'], ['k','l',-3], ['j',-2,'l']]\n",
        "order, log_cost, is_optimal = solve_order(dims, connects, max_branch=1000)\n",
        "\n",
        "# examine the outputs\n",
        "print('order determined from solver: ', order) \n",
        "cost_exact = int(2*(d**4)*chi)\n",
        "print('actual cost: %d, theory cost: %d' %(np.round(10**log_cost), cost_exact))\n",
        "print('guaranteed optimal: ', is_optimal) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLseGkbS0Dmn"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Examination of cost scaling\n",
        "\n",
        "Although `xcon` can return the numeric cost of a contraction given tensors of some specific dimensions, often we are more interested in knowing how this cost **scales** as a function of the index dimensions. To this end the `compute_costs` routine is useful; if provided a list of tensor shapes it will produce a list specifying individual cost of each of the pairwise contractions required to contract the network.\n",
        "\n",
        "Here the dims can be provided either as numeric values (as `int`) or symbolically (as `str`), or as a mixture of both. If symbolic dims are provided then symbolic costs are returned. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCmV9PpnvUkm"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "**Example 7: cost scaling analysis**\n",
        "\n",
        "Let us reconsider the cost scaling of network from Example 5 using the `compute_costs` routine. Recall that we determined analytically that this network required two pairwise contractions, each of cost ${d^4 \\chi}$, to contract. The `compute_costs` routine can reproduce this scaling result: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pseAh_ItqR2w"
      },
      "source": [
        "# Snip.(8): solve for costs\n",
        "\n",
        "# define the network and order\n",
        "connects = [[-1,'j','k'], ['k','l',-3], ['j',-2,'l']]\n",
        "order = ['k', 'j', 'l']\n",
        "\n",
        "# investing cost scaling as function of `d` and `chi`\n",
        "dims_symbolic = [('d','d','chi'),('chi','d','d'),('d','chi','d')]\n",
        "symbolic_costs = compute_costs(connects, order=order, dims=dims_symbolic)\n",
        "print('cost scaling: ', symbolic_costs)\n",
        "\n",
        "# investing cost scaling as function of `chi` with d=5 held fixed \n",
        "d = 5\n",
        "dims_mixed = [[d,d,'chi'],['chi',d,d],[d,'chi',d]]\n",
        "mixed_costs = compute_costs(connects, order=order, dims=dims_mixed)\n",
        "print('cost scaling: ', mixed_costs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwYIoKeTzrFy"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Network consistancy checking\n",
        "\n",
        "Often it is very easy to make a mistake when defining a complicated network. The `xcon` routine contains the ability to identify several types of error in network definitions, including mismatches in index dimensions and incorrectly labelled indices. This functionality is disabled by default (to minimize the computational overhead) but can be enabled by setting the optional input `perform_check=True`. The network checking function `check_network` can also be called separately outside of `xcon` if needed. Of course there are also many types of error that cannot be identified in the checker: just because a network passes the checks does not necessarily mean that it is the correct network that you intended!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yADpPfhW3nVx"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "**Example 8: identification of user error**\n",
        "\n",
        "Consider the network below, which is used in the optimization of a $1D$ variational MERA:\n",
        "<a name=\"fig5\"></a>\n",
        "<p align=\"center\">\n",
        "<img src='https://github.com/gevenbly/TensorAlgs/blob/main/images/net7.png?raw=true' width=300px > \n",
        "</p> \n",
        "<p align = \"right\">\n",
        "$(\\textrm{Fig}.5)$ \n",
        "</p>\n",
        "\n",
        "This network, while significantly more complex that the previous example networks considered in this tutorial, is typical of what may be encountered in a practical algorithm. Inputting this network into `xcon` is a tricky prospect; it would be very easy to make a small error in the network definition. In the code below we demonstrate how `xcon` can identify such an error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25Ic75e33_ly"
      },
      "source": [
        "# Snip.(9): network consistancy checking\n",
        "\n",
        "# initialize some random tensors\n",
        "chi = 4\n",
        "chimid = 3\n",
        "u = np.random.rand(4,4,3,3)\n",
        "w = np.random.rand(3,3,4)\n",
        "ham = np.random.rand(4,4,4,4,4,4)\n",
        "rho = np.random.rand(4,4,4,4,4,4)\n",
        "\n",
        "# define a network (from 1D binary MERA)\n",
        "tensors = [u,u,w,w,w,ham,u,u,w,w,w,rho]\n",
        "connects = [[1, 3, 10, 11], [4, 7, 12, 13], [8, 1, 21], [11, 12, 22],\n",
        "            [13, 14, 23], [2, 5, 6, 3, 4, 7], [1, 2, 9, 17], [5, 6, 16, 15],\n",
        "            [8, 9, 18], [17, 16, 19], [15, 14, 20], [18, 19, 20, 21, 22, 23]]\n",
        "order = [4,7,5,6,12,11,3,1,2,17,16,8,10,9,14,13,15,18,19,20,21,22,23]\n",
        "\n",
        "# the above network definition contains an error, can you spot it? No?\n",
        "# Then its a good thing that the checker in `xcon` can spot the error:\n",
        "tensor_out = xcon(tensors, connects, order=order, perform_check=True)\n",
        "\n",
        "# this returns the error message:\n",
        "# 'ValueError: Network definition error: more than two indices labelled `1`'\n",
        "#\n",
        "# it turns out that the third tensor was mislabelled with a `1` instead of `10`,\n",
        "# the correct labelling is: [8, 1, 21] -> [8, 10, 21]\n",
        "\n",
        "\n",
        "# alternatively we could have called the network checker on its own\n",
        "dims = [tensor.shape for tensor in tensors]\n",
        "check_network(connects, dims, order)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3pEWOIuBne0"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Network visualization\n",
        "\n",
        "We can make network diagrams in python by making use of the `draw_network` routine from the `network_renderer` module (see the [Guide](https://colab.research.google.com/drive/1TIWa_UApRIL8ZxJc16fyBKtO1IyhH3qN?usp=sharing) for full instructions). The renderer takes the same `connects` input as `xcon`, but can also accept extra optional arguments such as tensor `names` to further decorate the diagram. The render can also perform the same checks as the `check_network` routine and can display the same computational costs as `compute_costs`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSBWTneedFeT"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "**Example 9: network diagram in python**\n",
        "\n",
        "In this example we recreate the network from Fig.(4) using the network renderer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YmvwW6gB31O"
      },
      "source": [
        "# Snip.(10): network diagrams\n",
        "\n",
        "# define xcon info for the network from Fig.(4)\n",
        "dims = [['d','d','chi'],['chi','d','d'],['d','chi','d']]\n",
        "connects = [['i','j','k'], ['k','l','m'], ['j','n','l']]\n",
        "order = ['k','j','l']\n",
        "open_order = ['i','n','m']\n",
        "\n",
        "# define the tensor names\n",
        "names = ['A','B','C']\n",
        "\n",
        "# draw the networks \n",
        "fig = draw_network(connects, names=names, order=order, dims=dims, \n",
        "                   open_order=open_order, show_costs=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF89ktriyF5i"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Single-tensor environments\n",
        "\n",
        "Imagine that we have a closed network (i.e. a network with no open indices) comprised of a set of tensors $\\{A,B,C,\\ldots \\}$ which when contracted evaluates to a scalar $c$. We define an environment $\\Gamma_A$ as the tensor that would be obtained from contacting the network with tensor $A$ removed from it. It follows that the contraction of matching indices from tensor $A$ with its environment $\\Gamma_A$, which we denote using the tensor-trace '$\\textrm{tTr}$', evaluates to the scalar $c$,\n",
        "\n",
        "\\begin{equation}\n",
        "\\textrm{tTr}(A, \\Gamma_A) = c. \\tag{9}\n",
        "\\end{equation}\n",
        "\n",
        "The calculation of tensor environments, which may be understood as the derivative of the network with respect a single tensor, is often a key part of network optimization strategies. In many cases we may be interested varying the parameters contained in some tensor $A$ as to maximise (or minimise) the scalar $c$ associated to the closed network, which is easily achieved if the environment $\\Gamma_A$ is known.\n",
        "\n",
        "In Fig.(6) below we depict a closed network (relating to the optimization of a $1D$ binary MERA) and the enviornment of the $h$ from within this network. \n",
        "\n",
        "<a name=\"fig6\"></a>\n",
        "<p align=\"center\">\n",
        "<img src='https://github.com/gevenbly/TensorAlgs/blob/main/images/net8.png?raw=true' width=600px > \n",
        "</p> \n",
        "<p align = \"right\">\n",
        "$(\\textrm{Fig}.6)$ \n",
        "</p>\n",
        "\n",
        "In many tensor algorithms it is required to compute many different single tensor environments from the same network. For instance, the optimization algorithm for a $1D$ binary MERA requires one to compute a total of seven different environments from the network in Fig.(6). One could could these compute each environment separately by (i) defining a separate network for each environment, (ii) solving each network for the optimal contraction order, (iii) contracting each environment network separately. However, given the importance of the calculation of environments for tensor algorithms, the `xcon` routine has been designed to automate these task, without the need for the user to consider environment networks separately. \n",
        "\n",
        "Let us assume that we have a closed network of $N$ tensors with some known contraction order, such that it can be contracted to a scalar with cost $\\kappa$. Then, after setting the optional input `which_envs = r` for some intger $r < N$, the `xcon` routine will return the environment of the $r^\\textrm{th}$ tensor. Importantly `xcon` can automatically determine the proper contraction order for the environment, such that the contraction cost remains at $\\kappa$, even though the order required for the environment may be very different to the order required for the closed network.\n",
        "\n",
        "One can also set `which_envs` as a list of integers, e.g. `which_envs = [0, 3, 4]`, which will return a list of the corresponding tensor environments. It should be noted that computing multiple environments simultaneously is generally much more efficient than computing the same environments sequentially (i.e. one at a time), since `xcon` is able to reuse many of the intermediate tensors when contracting for multiple environments.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd8dyw8q22Vk"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "**Example 10: tensor environments**\n",
        "\n",
        "In this example we evaluate some tensor environments from the network in Fig.(6):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVAj6AfUPp6C"
      },
      "source": [
        "# Snip.(11): Tensor environments\n",
        "\n",
        "# define some random tensors\n",
        "chi = 4\n",
        "chimid = 3\n",
        "w = np.random.rand(chimid, chimid, chi)\n",
        "u = np.random.rand(chi,chi,chimid,chimid)\n",
        "h = np.random.rand(chi,chi,chi,chi,chi,chi)\n",
        "rho = np.random.rand(chi,chi,chi,chi,chi,chi)\n",
        "\n",
        "# define the closed network from Fig.(6)\n",
        "tensors = [u, u, w, w, w, h, u, u, w, w, w, rho]\n",
        "connects = [[1, 3, 10, 11], [4, 7, 12, 13], [8, 10, 21], [11, 12, 22],\n",
        "            [13, 14, 23], [2, 5, 6, 3, 4, 7], [1, 2, 9, 17], [5, 6, 16, 15],\n",
        "            [8, 9, 18], [17, 16, 19], [15, 14, 20], [18, 19, 20, 21, 22, 23]]\n",
        "\n",
        "# solve closed network for optimal order, then contract to a scalar \n",
        "scalar_out, order, cost = xcon(tensors, connects, return_info=True, \n",
        "                               solver='full')\n",
        "\n",
        "# evaluate the environment of the `h` tensor\n",
        "env_h = xcon(tensors, connects, order=order, which_envs=5)\n",
        "# check that the contraction of `h` with `env_h` matches the scalar\n",
        "scalar_out0 = np.inner(env_h.flatten(), h.flatten())\n",
        "print('scalars match: %r' %(np.allclose(scalar_out, scalar_out0)))\n",
        "\n",
        "# evaluate the environments of the each of the six `w` tensors \n",
        "envs_w = xcon(tensors, connects, order=order, which_envs=[2,3,4,8,9,10])\n",
        "# check that the contraction of `w` with each of the envs matches the scalar\n",
        "for env in envs_w:\n",
        "  scalar_temp = np.inner(env.flatten(), w.flatten())\n",
        "  print('scalars match: %r' %(np.allclose(scalar_out, scalar_temp)))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}